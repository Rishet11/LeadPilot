#!/usr/bin/env python3
"""
LeadPilot - AI Lead Generation Agent

A local-first Python agent that discovers small businesses 
with poor online presence and exports qualified leads.

Usage:
    python main.py                          # Use config.json defaults
    python main.py --city Mumbai --category Bakery --limit 50
    python main.py --dry-run                # Test with demo data
    python main.py --check-websites         # Verify website accessibility
    python main.py --ai-summary             # Add Gemini AI summaries
    python main.py --agent                  # Enable agentic AI mode
"""

import os
import sys
import json
import argparse
from datetime import datetime

from dotenv import load_dotenv

# Load environment variables
load_dotenv()


def load_config(config_path: str = "config.json") -> dict:
    """Load configuration from JSON file."""
    default_config = {
        "city": "Delhi",
        "category": "Gym", 
        "limit": 100,
        "export": {"csv": True, "google_sheets": False},
        "ai_summary": {"enabled": False}
    }
    
    try:
        with open(config_path, "r") as f:
            config = json.load(f)
            # Merge with defaults
            for key, value in default_config.items():
                if key not in config:
                    config[key] = value
            return config
    except FileNotFoundError:
        print(f"‚ö†Ô∏è  Config file not found, using defaults")
        return default_config


def run_pipeline(city: str, category: str, limit: int, 
                 dry_run: bool = False, check_websites: bool = False,
                 ai_summary: bool = False, google_sheets: bool = False,
                 agent_mode: bool = False):
    """
    Run the complete lead generation pipeline.
    
    Args:
        city: Target city
        category: Business category
        limit: Maximum results
        dry_run: Use demo data instead of API
        check_websites: Verify website accessibility
        ai_summary: Add AI-powered summaries
        google_sheets: Export to Google Sheets
        agent_mode: Use agentic AI for autonomous lead evaluation
    """
    from apify_client import (
        run_google_maps_scraper, poll_run_status, 
        fetch_dataset, save_raw_data, get_demo_data
    )
    from cleaner import clean_dataframe, add_derived_columns
    from scorer import score_dataframe, load_config as load_scoring_config
    from exporter import export_csv, export_google_sheets, print_summary
    
    print("\n" + "="*50)
    print("üöÄ LEADPILOT - Lead Generation Agent")
    print("="*50)
    print(f"üìç City: {city}")
    print(f"üìÇ Category: {category}")
    print(f"üìä Limit: {limit}")
    print(f"üîß Mode: {'Demo' if dry_run else 'Live API'}")
    print("="*50 + "\n")
    
    # Step 1: Fetch data
    if dry_run:
        print("üì• Using demo data (dry run mode)...")
        raw_data = get_demo_data()
    else:
        print("üì• Fetching data from Apify...")
        try:
            result = run_google_maps_scraper(city, category, limit)
            print(f"  Run ID: {result['run_id']}")
            print(f"  Dataset ID: {result['dataset_id']}")
            
            print("‚è≥ Waiting for scraper to complete...")
            status = poll_run_status(result['run_id'])
            
            if status != "SUCCEEDED":
                print(f"‚ùå Scraping failed with status: {status}")
                sys.exit(1)
            
            print("üì¶ Fetching results...")
            raw_data = fetch_dataset(result['dataset_id'])
            
            # Save raw data
            save_raw_data(raw_data)
            
        except Exception as e:
            print(f"‚ùå Error fetching data: {e}")
            print("üí° Tip: Run with --dry-run to test without API")
            sys.exit(1)
    
    print(f"‚úÖ Retrieved {len(raw_data)} results")
    
    # Step 2: Clean data
    print("\nüßπ Cleaning and normalizing data...")
    df = clean_dataframe(raw_data)
    df = add_derived_columns(df)
    print(f"‚úÖ Cleaned data: {len(df)} unique leads")
    
    # Step 3: Score leads
    print("\nüìä Scoring leads...")
    scoring_config = load_scoring_config()
    df = score_dataframe(df, scoring_config, check_websites=check_websites)
    print(f"‚úÖ Scored {len(df)} leads")
    
    # Step 4: AI summaries (optional)
    if ai_summary and not agent_mode:
        print("\nü§ñ Generating AI summaries...")
        try:
            from ai_summary import add_ai_summaries
            df = add_ai_summaries(df, max_leads=10)
        except Exception as e:
            print(f"‚ö†Ô∏è  AI summary failed: {e}")
    
    # Step 4b: Agentic AI mode (autonomous evaluation)
    if agent_mode:
        print("\nü§ñ Running Agentic AI pipeline...")
        try:
            from lead_agent import run_agent_pipeline
            df = run_agent_pipeline(df, max_leads=10)
        except Exception as e:
            print(f"‚ö†Ô∏è  Agent mode failed: {e}")
            import traceback
            traceback.print_exc()
    
    # Step 5: Export
    print("\nüíæ Exporting leads...")
    
    # Always export CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = f"data/leads_{timestamp}.csv"
    export_csv(df, csv_path)
    
    # Also save as latest
    export_csv(df, "data/leads.csv")
    
    # Google Sheets (optional)
    if google_sheets:
        try:
            sheet_url = export_google_sheets(df)
            print(f"üìä Google Sheets: {sheet_url}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Google Sheets export failed: {e}")
    
    # Print summary
    print_summary(df)
    
    print("\n‚úÖ Pipeline complete!")
    print(f"üìÅ Output: {csv_path}")
    
    return df


def main():
    """Main entry point with CLI argument parsing."""
    parser = argparse.ArgumentParser(
        description="LeadPilot - AI Lead Generation Agent",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                                  # Use config.json
  python main.py --city Mumbai --category Bakery  # Override city/category
  python main.py --dry-run                        # Test with demo data
  python main.py --ai-summary                     # Add AI summaries
        """
    )
    
    parser.add_argument("--city", type=str, help="Target city")
    parser.add_argument("--category", type=str, help="Business category")
    parser.add_argument("--limit", type=int, help="Maximum results")
    parser.add_argument("--dry-run", action="store_true", 
                        help="Use demo data instead of API")
    parser.add_argument("--check-websites", action="store_true",
                        help="Verify website accessibility (slower)")
    parser.add_argument("--ai-summary", action="store_true",
                        help="Add AI-powered summaries (requires Gemini API)")
    parser.add_argument("--google-sheets", action="store_true",
                        help="Export to Google Sheets")
    parser.add_argument("--agent", action="store_true",
                        help="Enable agentic AI mode (autonomous lead evaluation)")
    parser.add_argument("--config", type=str, default="config.json",
                        help="Path to config file")
    
    args = parser.parse_args()
    
    # Load config
    config = load_config(args.config)
    
    # Override with CLI args
    city = args.city or config.get("city", "Delhi")
    category = args.category or config.get("category", "Gym")
    limit = args.limit or config.get("limit", 100)
    
    # Run pipeline
    run_pipeline(
        city=city,
        category=category,
        limit=limit,
        dry_run=args.dry_run,
        check_websites=args.check_websites,
        ai_summary=args.ai_summary or config.get("ai_summary", {}).get("enabled", False),
        google_sheets=args.google_sheets or config.get("export", {}).get("google_sheets", False),
        agent_mode=args.agent
    )


if __name__ == "__main__":
    main()
